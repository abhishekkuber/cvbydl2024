{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni8AJi1A84wz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import shutil\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from google.colab import drive\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from PIL import Image\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from skimage import color\n",
        "from skimage.color import lab2rgb, rgb2lab\n",
        "from skimage.transform import resize\n",
        "from skimage import io\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZBrHylg-Fws"
      },
      "outputs": [],
      "source": [
        "drive.mount('/content/drive/', force_remount=True)\n",
        "print(\"Files in the current directory:\")\n",
        "print(os.listdir(\"/content/drive/MyDrive/TUDelft/Seminar_Computer_Vision/CVbyDL/DATA/rescaled\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kqWFA61qnwuU"
      },
      "outputs": [],
      "source": [
        "# Sometimes Google Drive takes a long time to read / know the number of files in a directory. Therefore, we get the zipped files, and unzip them per session, so that there is no problem of missing data.\n",
        "\n",
        "# Define the path to the ZIP files and corresponding target subdirectories\n",
        "zip_files = {\n",
        "    \"/content/drive/MyDrive/TUDelft/Seminar_Computer_Vision/CVbyDL/DATA/rescaled/test/input-test-set-rescaled.zip\": \"/content/unzipped_data/test/input\",\n",
        "    \"/content/drive/MyDrive/TUDelft/Seminar_Computer_Vision/CVbyDL/DATA/rescaled/train/input-train-set-rescaled.zip\": \"/content/unzipped_data/train/input\",\n",
        "}\n",
        "\n",
        "# Ensure base destination folders exist\n",
        "if not os.path.exists(\"/content/unzipped_data/train\"):\n",
        "    os.makedirs(\"/content/unzipped_data/train\")\n",
        "if not os.path.exists(\"/content/unzipped_data/test\"):\n",
        "    os.makedirs(\"/content/unzipped_data/test\")\n",
        "\n",
        "# Unzip the files into specific folders stripping the top directory\n",
        "for zip_path, extraction_path in tqdm(zip_files.items()):\n",
        "    # Ensure each specific extraction path exists\n",
        "    if not os.path.exists(extraction_path):\n",
        "        os.makedirs(extraction_path)\n",
        "\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        # We filter out the first level of the directory\n",
        "        for file_info in zip_ref.infolist():\n",
        "            # Skip directories at the root level in the zip file\n",
        "            if file_info.filename.count('/') == 1 and file_info.is_dir():\n",
        "                continue\n",
        "            # Construct the correct path by stripping the first directory\n",
        "            new_file_path = os.path.join(extraction_path, '/'.join(file_info.filename.split('/')[1:]))\n",
        "            new_file_dir = os.path.dirname(new_file_path)\n",
        "            if not os.path.exists(new_file_dir):\n",
        "                os.makedirs(new_file_dir)\n",
        "            if not file_info.is_dir():  # Avoid trying to open directories as files\n",
        "                with zip_ref.open(file_info) as source, open(new_file_path, 'wb') as target:\n",
        "                    target.write(source.read())\n",
        "\n",
        "# Print the count of files in each directory for verification\n",
        "for category, path in zip_files.items():\n",
        "    print(f\"Files in {path} directory:\")\n",
        "    print(len([name for name in os.listdir(path) if os.path.isfile(os.path.join(path, name))]))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UfiRQvlHSbsE"
      },
      "outputs": [],
      "source": [
        "DATASET_LOCATION = \"/content/unzipped_data\"\n",
        "UNIQUE_ID = \"original_architecture\"\n",
        "OUT_LOCATION = f\"/content/drive/MyDrive/TUDelft/Seminar_Computer_Vision/CVbyDL/the_experiment/{UNIQUE_ID}\"\n",
        "print(\"UNIQUE_ID : \",UNIQUE_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u63zuMojI15t"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zhW7WRtDXhpp"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# If you want to reduce the number of feature maps per layer, just divide the in and out channels for Conv2d by powers of two (for the blog we have 1/8 and 1/16)\n",
        "\n",
        "# NOTE : You need to rescale all the input images to 224x224!\n",
        "\n",
        "# Shared Low Level Features\n",
        "class SLLF(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        # Output channels for each convolution layer are 64, 128, 128, 256, 256, 512. See table 1 for the details\n",
        "\n",
        "        # For conv1, conv3, conv5, stride is 2. Therefore, it halves the height and width of the image.\n",
        "        super(SLLF, self).__init__()\n",
        "\n",
        "        # Input image = 224x224\n",
        "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, stride=2, padding=1) # 224x224 -> 112x112\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1) # 112x112 -> 112x112\n",
        "        self.bn2 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=2, padding=1) # 112x112 -> 56x56\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1) # 56x56 -> 56x56\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=2, padding=1) # 56x56 -> 28x28\n",
        "        self.bn5 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.conv6 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1) # 28x28 -> 28x28\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.relu(self.bn4(self.conv4(x)))\n",
        "        x = self.relu(self.bn5(self.conv5(x)))\n",
        "        x = self.relu(self.conv6(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "# Global Image Features\n",
        "class GIF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(GIF, self).__init__()\n",
        "\n",
        "        # Input image = 28x28\n",
        "        self.conv1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1) # 28x28 -> 14x14\n",
        "        self.bn1 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1) # 14x14 -> 14x14\n",
        "        self.bn2 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=2, padding=1) # 14x14 -> 7x7\n",
        "        self.bn3 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1) # 7x7 -> 7x7\n",
        "\n",
        "        # Input to this layer is a feature map of dimension 512x7x7 = 25088\n",
        "        # To pass it into the linear layers, you need to flatten the feature map\n",
        "        self.fc1 = nn.Linear(in_features=25088, out_features=1024)\n",
        "        self.fc2 = nn.Linear(in_features=1024, out_features=512)\n",
        "        self.fc3 = nn.Linear(in_features=512, out_features=256)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.relu(self.conv4(x))\n",
        "\n",
        "        # print(f\"Printing from the GLF forward function\\nDimensions before flattening {x.shape}\")\n",
        "\n",
        "        # After all convolutions, flatten the input before passing them to the Fully Connected layers\n",
        "        x = torch.flatten(x, 1)\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "\n",
        "        # Output to the fustion layer\n",
        "        x = self.relu(self.fc3(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "# Mid Level Features\n",
        "class MLF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLF, self).__init__()\n",
        "\n",
        "        # Input image = 28x28\n",
        "        self.conv1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, stride=1, padding=1) # 28x28 -> 28x28\n",
        "        self.bn1 = nn.BatchNorm2d(512)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1) # 28x28 -> 28x28\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.relu(self.conv2(x))\n",
        "        return x\n",
        "\n",
        "\n",
        "class ColorizationNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ColorizationNetwork, self).__init__()\n",
        "\n",
        "        # Input image = 28x28\n",
        "        self.conv1 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1, padding=1) # 28x28 -> 28x28\n",
        "        self.bn1 = nn.BatchNorm2d(128)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(in_channels=128, out_channels=64, kernel_size=3, stride=1, padding=1) # 56x56 -> 56x56\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1) # 56x56 -> 56x56\n",
        "        self.bn3 = nn.BatchNorm2d(64)\n",
        "\n",
        "        self.conv4 = nn.Conv2d(in_channels=64, out_channels=32, kernel_size=3, stride=1, padding=1) # 112x112 -> 112x112\n",
        "        self.bn4 = nn.BatchNorm2d(32)\n",
        "\n",
        "        self.conv5 = nn.Conv2d(in_channels=32, out_channels=2, kernel_size=3, stride=1, padding=1) # 112x112 -> 112x112\n",
        "\n",
        "\n",
        "        self.upsample1 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.upsample2 = nn.Upsample(scale_factor=2, mode='nearest')\n",
        "        self.upsample3 = nn.Upsample(scale_factor=2, mode='nearest') # Used after the sigmoid layer\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "\n",
        "    # Input is the output of the fusion layer!\n",
        "    # Vector is of dimensions 256x28x28\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.bn1(self.conv1(x)))\n",
        "        x = self.upsample1(x)\n",
        "\n",
        "        x = self.relu(self.bn2(self.conv2(x)))\n",
        "        x = self.relu(self.bn3(self.conv3(x)))\n",
        "        x = self.upsample2(x)\n",
        "\n",
        "        x = self.relu(self.bn4(self.conv4(x)))\n",
        "        # Output layer\n",
        "        x = self.sigmoid(self.conv5(x))\n",
        "\n",
        "        x = self.upsample3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class FusionLayer(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FusionLayer, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=512, out_channels=256, kernel_size=3, stride=1, padding=1) # 28x28 -> 28x28\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, glf, mlf):\n",
        "        # Mid Out : torch.Size([2, 256, 28, 28])\n",
        "        # Glob Out : torch.Size([2, 256])\n",
        "        batch_size = glf.shape[0]\n",
        "        glf = glf.unsqueeze(-1).unsqueeze(-1)\n",
        "        glf = glf.expand(batch_size, 256, 28, 28)\n",
        "        fused = torch.cat((mlf, glf), 1)\n",
        "\n",
        "        # fused : torch.Size([2, 512, 28, 28])\n",
        "        fused = self.relu(self.conv1(fused))\n",
        "        return fused"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6x5e-V9IukT"
      },
      "source": [
        "### With Global Features Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGq-BtB4JOD5"
      },
      "outputs": [],
      "source": [
        "class FullNetworkGLF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullNetworkGLF, self).__init__()\n",
        "        self.sllf = SLLF()\n",
        "        self.glf = GIF()\n",
        "        self.mlf = MLF()\n",
        "        self.fusionLayer = FusionLayer()\n",
        "        self.colorizationNetwork = ColorizationNetwork()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        llf = self.sllf.forward(x)\n",
        "        mlf = self.mlf.forward(llf)\n",
        "        glf = self.glf.forward(llf)\n",
        "        fused = self.fusionLayer.forward(glf, mlf)\n",
        "        predicted_colors = self.colorizationNetwork.forward(fused)\n",
        "\n",
        "        return predicted_colors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uY41b7aT4UZT"
      },
      "outputs": [],
      "source": [
        "model = FullNetworkGLF()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOULKAxlI1K_"
      },
      "source": [
        "### Architecture (without Global Features Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OjJ9WbeZJGmR"
      },
      "outputs": [],
      "source": [
        "class FullNetworkNoGLF(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(FullNetworkNoGLF, self).__init__()\n",
        "        self.sllf = SLLF()\n",
        "        self.mlf = MLF()\n",
        "        self.colorizationNetwork = ColorizationNetwork()\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        llf = self.sllf.forward(x)\n",
        "        mlf = self.mlf.forward(llf)\n",
        "        predicted_colors = self.colorizationNetwork.forward(mlf)\n",
        "\n",
        "        return predicted_colors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65lDovBJJjHl"
      },
      "outputs": [],
      "source": [
        "model = FullNetworkNoGLF()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dcBljXOiJONH"
      },
      "source": [
        "# Dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R8td1VDGJPtJ"
      },
      "outputs": [],
      "source": [
        "class FilmPicturesDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.imgs = self.make_dataset()\n",
        "\n",
        "    def make_dataset(self):\n",
        "        images = []\n",
        "        valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.gif', '.tiff') # Doing this because it was getting confused with the .DS_Store file\n",
        "        for img_name in os.listdir(self.root_dir):\n",
        "            if img_name.endswith(valid_extensions):\n",
        "                img_path = os.path.join(self.root_dir, img_name)\n",
        "                images.append(img_path)\n",
        "        return images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.imgs[idx]\n",
        "        l, ground_truth_a_b = rgb_to_normalized_lab(img_path)\n",
        "        return l, ground_truth_a_b\n",
        "\n",
        "def convert_to_grayscale(image_path):\n",
        "    '''\n",
        "    Takes an RGB image, and then just converting it to grayscale\n",
        "    '''\n",
        "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
        "    img = torch.tensor(img)\n",
        "    # adding a channel dimension: 1 x H x W\n",
        "    img = img.unsqueeze(0)  #pytorch expects channel first\n",
        "    return img\n",
        "\n",
        "def rgb_to_normalized_lab(image_path):\n",
        "    '''\n",
        "    Takes as input an image, and then converts it into the normalized Lab color scheme\n",
        "\n",
        "    '''\n",
        "\n",
        "    # (224, 224, 3) <class 'numpy.ndarray'>\n",
        "    img = io.imread(image_path)\n",
        "\n",
        "    # Just in case image is not the correct dimensions\n",
        "    img = resize(img, (224, 224))\n",
        "\n",
        "    # (224, 224, 3) <class 'numpy.ndarray'>\n",
        "    img_lab = rgb2lab(img)\n",
        "\n",
        "    # LAB range L: 0-100, a: -127-128, b: -128-127.\n",
        "    img_lab[:,:,:1] = img_lab[:, :, :1] / 100.0\n",
        "    img_lab[:,:,1:] = (img_lab[:, :, 1:] + 128.0) / 256.0\n",
        "\n",
        "    # (224, 224, 3) <class 'numpy.ndarray'>\n",
        "    img_lab = np.transpose(img_lab, (2,0,1)).astype(np.float32)\n",
        "    img_lab = torch.from_numpy(img_lab)\n",
        "    # shape (3, 224, 224), torch.Tensor\n",
        "\n",
        "    luminance = img_lab[:1,:,:] # Use [:1] instead of [0] because [0] drops the first dimension (Luminance becomes (224,224), whereas we want it (1,224,224))\n",
        "    ab = img_lab[1:,:,:]\n",
        "\n",
        "    return luminance, ab\n",
        "\n",
        "def lab_to_rgb(luminance, ab):\n",
        "    '''\n",
        "    Converts and unnormalizes the Lab image to RGB\n",
        "    '''\n",
        "    luminance = luminance.numpy() * 100.0\n",
        "    ab = (ab.numpy() * 255.0) - 128.0\n",
        "\n",
        "    # torch tensor of shape (batch_size, 3, 224, 224)\n",
        "    luminance = luminance.transpose((1, 2, 0))\n",
        "    ab = ab.transpose((1, 2, 0))\n",
        "\n",
        "    # skimage requires the images to be of the shape (batch_size, height, width, channels)\n",
        "    img_stack = np.dstack((luminance, ab))\n",
        "    img_stack = img_stack.astype(np.float64)\n",
        "\n",
        "    return lab2rgb(img_stack)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNVatzBYJ6OG"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wa-w2gnrJ7fY"
      },
      "outputs": [],
      "source": [
        "def model_prediction_to_rgb(luminance, ab_pred):\n",
        "    '''\n",
        "    Takes as output the original luminance channel, and the predicted ab channels from the model, and joins them to convert to RGB\n",
        "    Used for viewing the outputs during model inference\n",
        "    '''\n",
        "\n",
        "    # Bringing them back to the original range\n",
        "    luminance = luminance.numpy() * 100.0\n",
        "    ab_pred = ab_pred.numpy() * 254.0 - 127.0\n",
        "\n",
        "    # Currently, the ordering is CxHxW\n",
        "    # We need to transpose axes back to HxWxC\n",
        "    luminance = luminance.transpose((1, 2, 0))\n",
        "    ab_pred = ab_pred.transpose((1, 2, 0))\n",
        "\n",
        "    img_stack = np.dstack((luminance, ab_pred))\n",
        "    img_stack = img_stack.astype(np.float64)\n",
        "\n",
        "    return  color.lab2rgb(img_stack)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-fBo5pWBJBI4"
      },
      "source": [
        "# Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acqYqYlqIsA3"
      },
      "outputs": [],
      "source": [
        "class TrainingLoop:\n",
        "\n",
        "    def __init__(self, batch_size, epochs, train_dir, val_dir, test_dir, start_epoch=0):\n",
        "        '''\n",
        "        Initializes the datasets according to the directories provided.\n",
        "        Also creates dataloaders.\n",
        "        Initializes the model based on the hyperparameters given.\n",
        "\n",
        "        Start epoch is added because it enables to start training from a certain epoch.\n",
        "        Needed as we usually could not train the model in one go on Colab GPUs\n",
        "        '''\n",
        "\n",
        "\n",
        "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "        print(f\"Using device : {self.device}\")\n",
        "\n",
        "\n",
        "        self.train_dir = train_dir\n",
        "        self.val_dir = val_dir\n",
        "        self.test_dir = test_dir\n",
        "\n",
        "        self.trainset = FilmPicturesDataset(self.train_dir)\n",
        "        self.testset = FilmPicturesDataset(self.test_dir)\n",
        "        self.trainloader = DataLoader(dataset=self.trainset, batch_size=self.batch_size, shuffle=True)\n",
        "        self.testloader = DataLoader(dataset=self.testset, batch_size=self.batch_size, shuffle=False)\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.mse = nn.MSELoss(reduction='sum')\n",
        "        self.start_epoch = start_epoch\n",
        "        self.epochs = epochs\n",
        "        self.optimizer = optim.Adadelta(self.net.parameters())\n",
        "\n",
        "        self.output_dir = OUT_LOCATION\n",
        "\n",
        "        # self.net = FullNetworkNoGLF()\n",
        "        self.net = FullNetworkGLF()\n",
        "        self.net.to(self.device)\n",
        "\n",
        "\n",
        "    def train(self, epoch):\n",
        "        '''\n",
        "        Trains the model for one epoch\n",
        "        '''\n",
        "        epoch_loss = 0.0\n",
        "\n",
        "        # Setting the model to train mode\n",
        "        self.net.train()\n",
        "\n",
        "        for batch_no, img in enumerate(self.trainloader):\n",
        "            self.optimizer.zero_grad()\n",
        "\n",
        "            luminance, ab = img\n",
        "            luminance, ab = luminance.to(self.device), ab.to(self.device)\n",
        "\n",
        "            ab_pred = self.net(luminance)\n",
        "            loss = self.mse(ab, ab_pred)\n",
        "\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            batch_loss = loss.item()\n",
        "\n",
        "            print(f'Epoch {epoch+1} / {self.epochs} | Batch Number : {batch_no + 1} / {len(self.trainloader)} -> Batch Loss : {batch_loss}')\n",
        "            epoch_loss += batch_loss\n",
        "\n",
        "        epoch_loss /= len(self.trainloader)\n",
        "\n",
        "        # Save the model every 20 epochs. Required as a lot of times the runtime gets disconnected due to inactivity or running out of compute hours\n",
        "        if (epoch+1)%20 == 0:\n",
        "          model_folder = f\"{OUT_LOCATION}/models_saved\"\n",
        "          os.makedirs(model_folder, exist_ok=True)\n",
        "          model_path = os.path.join(model_folder, f\"{UNIQUE_ID}_model_epoch{epoch}.pt\")\n",
        "          torch.save(self.net.state_dict(), model_path)\n",
        "          print(f\"Model saved to : {model_path}\")\n",
        "\n",
        "        print(f\"Epoch loss: {epoch_loss}\")\n",
        "\n",
        "\n",
        "    def test(self, show_image=False):\n",
        "        '''\n",
        "        Inference on the images\n",
        "        If show_image = True, you also get the predicted images as the output\n",
        "        '''\n",
        "\n",
        "        self.net.to(self.device)\n",
        "\n",
        "        # Setting the model to evaluation mode\n",
        "        self.net.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_no, img in enumerate(self.testloader):\n",
        "\n",
        "                luminance, _ = img\n",
        "                luminance = luminance.to(self.device)\n",
        "                ab_pred= self.net(luminance)\n",
        "\n",
        "                luminance = luminance.to(torch.device(\"cpu\"))\n",
        "                ab_pred = ab_pred.to(torch.device(\"cpu\"))\n",
        "\n",
        "                for i in range(luminance.shape[0]):\n",
        "                    img = model_prediction_to_rgb(luminance[i], ab_pred[i])\n",
        "\n",
        "                    img *= 255.0\n",
        "                    img = img.astype(np.uint8)\n",
        "                    io.imsave(os.path.join(self.output_dir, f\"{batch_no}_{i}.png\"), img)\n",
        "                    if show_image:\n",
        "                      pil_img = Image.fromarray(img)\n",
        "                      display(pil_img)  # Display one image at a time in the notebook\n",
        "\n",
        "                print(f\"Batch {batch_no + 1} / {len(self.trainloader)}\")\n",
        "\n",
        "        print(\"Saved all photos to \" + self.output_dir)\n",
        "\n",
        "    # Trains the model for specified number of epochs, and then tests it\n",
        "    def run(self):\n",
        "        for epoch in range(self.start_epoch, self.epochs):\n",
        "            print(f\"Epoch : {epoch + 1} / {self.epochs}\")\n",
        "            self.train(epoch)\n",
        "        self.test()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6JmVOXmNlnHX"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LqJz0wk2Hf6h"
      },
      "outputs": [],
      "source": [
        "training_loop = TrainingLoop(batch_size=16, epochs=100, train_dir=f\"{DATASET_LOCATION}/train/input\", val_dir=f\"{DATASET_LOCATION}/test/input\", test_dir=f\"{DATASET_LOCATION}/test/input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j-jo0ApMJk8-"
      },
      "outputs": [],
      "source": [
        "training_loop.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9zs5cwHWfDGa"
      },
      "source": [
        "# Loading from checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_VwW_UpC9DH"
      },
      "source": [
        "Used in 2 cases\n",
        "\n",
        "1. Training more (Continuing training)\n",
        "2. Testing on certain images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN9GXyD0fDGa"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = f\"{OUT_LOCATION}/models_saved/model.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cog2LamLfivJ"
      },
      "source": [
        "### Continuing training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gm9wO5MOLty2"
      },
      "outputs": [],
      "source": [
        "# Run all the cells till main (do not run main)\n",
        "start_epoch = 20 # look at the latest model , epoch + 1\n",
        "end_epoch = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvvUT-U9fK1h"
      },
      "outputs": [],
      "source": [
        "training_loop = TrainingLoop(batch_size=16, epochs=(end_epoch-start_epoch), train_dir=f\"{DATASET_LOCATION}/train/input\", val_dir=f\"{DATASET_LOCATION}/test/input\", test_dir=f\"{DATASET_LOCATION}/test/input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5JOFElsThYRJ"
      },
      "outputs": [],
      "source": [
        "trainer.net.load_state_dict(torch.load(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "JgBKbnk0hpm7"
      },
      "outputs": [],
      "source": [
        "trainer.run()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndpiUxE4fnHk"
      },
      "source": [
        "### Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mUw6JE6OhtN"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = f\"{OUT_LOCATION}/models_saved/model.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K91fmIlcKw1X"
      },
      "outputs": [],
      "source": [
        "training_loop = TrainingLoop(batch_size=16, epochs=0, train_dir=f\"{DATASET_LOCATION}/train/input\", val_dir=f\"{DATASET_LOCATION}/test/input\", test_dir=f\"{DATASET_LOCATION}/test/input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X6qcZ7rKK_ZN"
      },
      "outputs": [],
      "source": [
        "trainer.net.load_state_dict(torch.load(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "seH1h-LZqFAY"
      },
      "outputs": [],
      "source": [
        "trainer.test()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du0brSK-bkfB"
      },
      "source": [
        "\n",
        "## Test speific images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4nhYOZ_fbvpm"
      },
      "outputs": [],
      "source": [
        "training_loop = TrainingLoop(batch_size=16, epochs=10, train_dir=f\"{DATASET_LOCATION}/train/input\", val_dir=f\"{DATASET_LOCATION}/test/Cinema\", test_dir=f\"{DATASET_LOCATION}/test/input\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SixuMRXKc1zg"
      },
      "outputs": [],
      "source": [
        "trainer.net.load_state_dict(torch.load(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFmggPsbcJNa"
      },
      "outputs": [],
      "source": [
        "trainer.test(show_image=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "u63zuMojI15t",
        "dcBljXOiJONH",
        "LNVatzBYJ6OG",
        "6JmVOXmNlnHX",
        "9zs5cwHWfDGa",
        "cog2LamLfivJ",
        "ndpiUxE4fnHk",
        "Du0brSK-bkfB"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
